{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this lab session is to test the performance of some usual bandit algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $k$ possible actions, $a \\in \\{ 0, 1,...,k - 1\\}$. \n",
    "\n",
    "We consider the following algorithms:\n",
    "* $\\varepsilon$-greedy\n",
    "* adaptive greedy\n",
    "* UCB\n",
    "* Thompson sampling\n",
    "\n",
    "Each algorithm returns an action based on the following inputs:\n",
    "\n",
    "| Variable   |      Type      |  Description |\n",
    "|:---|:---|:---|\n",
    "| `cum_rewards` |    np.ndarray    |   cumulative reward of each action (R) |\n",
    "| `nb_tries` |  np.ndarray  | number of tries of each action (N) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do:**\n",
    "* Code the UCB algorithm. \n",
    "* Observe the behaviour of the algorithms, for different parameters.\n",
    "* Test the principle of \"optimism in face of uncertainty\" on the greedy policies.\n",
    "\n",
    "**Note:** \n",
    "* Use the `simple_test` function to test the behaviour of the algorithms.\n",
    "* You can enforce optimism by changing the initial values of R and N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(cum_rewards, nb_tries, eps = 0.1):\n",
    "    k = len(nb_tries)\n",
    "    active = np.where(nb_tries > 0)[0]\n",
    "    if len(active) == 0 or np.random.random() < eps:\n",
    "        return np.random.randint(k)\n",
    "    else: \n",
    "        rewards = cum_rewards[active] / nb_tries[active]\n",
    "        best_actions = np.where(rewards == np.max(rewards))[0]\n",
    "        return active[np.random.choice(best_actions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_greedy(cum_rewards, nb_tries, c = 1):\n",
    "    k = len(nb_tries)\n",
    "    active = np.where(nb_tries > 0)[0]\n",
    "    t = np.sum(nb_tries)\n",
    "    if len(active) == 0 or np.random.random() < c / (c + t):\n",
    "        return np.random.randint(k)\n",
    "    else:\n",
    "        rewards = cum_rewards[active] / nb_tries[active]\n",
    "        best_actions = np.where(rewards == np.max(rewards))[0]\n",
    "        return active[np.random.choice(best_actions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb(cum_rewards, nb_tries, c = 1):\n",
    "            \n",
    "    k = len(nb_tries)\n",
    "    \n",
    "    active = np.where(nb_tries == 0)[0]   #tant qu'on a des zeros on explore les cas, pour gerer la situation du log au denominateur\n",
    "    \n",
    "    if(len(active) != 0):\n",
    "        # take the a random\n",
    "        return np.random.choice(active)\n",
    "    \n",
    "    else :\n",
    "        t = np.sum(nb_tries)\n",
    "        logt_n = np.log(t)/nb_tries\n",
    "        rewards = cum_rewards[active] / nb_tries[active]\n",
    "        best_actions =np.argmax(cum_rewards/nb_tries + c*np.sqrt(logt_n))\n",
    "        #best_actions = np.where(rewards == np.max(rewards + c*np.sqrt(np.log(t)/nb_tries))[0])\n",
    "        return best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thompson(cum_rewards, nb_tries):\n",
    "    try:\n",
    "        # Beta prior (binary rewards)\n",
    "        samples = np.random.beta(cum_rewards + 1, nb_tries - cum_rewards + 1)\n",
    "    except:\n",
    "        # Normal prior\n",
    "        samples = np.random.normal(cum_rewards / (nb_tries + 1), np.sqrt(1. / (nb_tries + 1)))\n",
    "    return np.argmax(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(algo, cum_rewards, nb_tries):\n",
    "    if algo == \"eps_greedy\":\n",
    "        return eps_greedy(cum_rewards, nb_tries)\n",
    "    elif algo == \"adaptive_greedy\":\n",
    "        return adaptive_greedy(cum_rewards, nb_tries)\n",
    "    elif algo == \"ucb\":\n",
    "        return ucb(cum_rewards, nb_tries)\n",
    "    elif algo == \"thompson\":\n",
    "        return thompson(cum_rewards, nb_tries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bernoulli_reward(a, model_param):\n",
    "    return float(np.random.random() < model_param[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test with binary rewards\n",
    "\n",
    "def simple_test(algo, model_param = [0.6, 0.3, 0.1], time_horizon = 10, nb_tries = None, cum_rewards = None):\n",
    "    k = len(model_param)\n",
    "    if nb_tries is None:\n",
    "        nb_tries = np.zeros(k, int)\n",
    "    if cum_rewards is None:\n",
    "        cum_rewards = np.zeros(k, float)\n",
    "    print(\"Parameters = \", model_param)\n",
    "    print (\"action -> reward\")\n",
    "    for t in range(time_horizon):\n",
    "        a = get_action(algo, cum_rewards, nb_tries)\n",
    "        r = get_bernoulli_reward(a, model_param)\n",
    "        print(str(a) + \" -> \" + str(int(r)))\n",
    "        cum_rewards[a] += r\n",
    "        nb_tries[a] += 1\n",
    "    active = np.where(nb_tries > 0)[0]\n",
    "    rewards = cum_rewards[active] / nb_tries[active]\n",
    "    best_actions = np.where(rewards == np.max(rewards))[0]\n",
    "    print(\"Best action (estimation of the algorithm) = \", active[best_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = [\"eps_greedy\", \"adaptive_greedy\", \"ucb\", \"thompson\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters =  [0.6, 0.3, 0.1]\n",
      "action -> reward\n",
      "0 -> 1\n",
      "1 -> 1\n",
      "2 -> 0\n",
      "0 -> 0\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "0 -> 1\n",
      "0 -> 1\n",
      "0 -> 0\n",
      "Best action (estimation of the algorithm) =  [1]\n"
     ]
    }
   ],
   "source": [
    "algo = algos[2]\n",
    "simple_test(algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regret and precision\n",
    "\n",
    "We now compare the performance of the algorithms in terms of **regret** and **precision**.\n",
    "\n",
    "We consider two models: Bernoulli rewards and normal rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(a, model, model_param):\n",
    "    if model == \"bernoulli\":\n",
    "        return float(np.random.random() < model_param[a])\n",
    "    elif model == \"normal\":\n",
    "        return np.random.normal(*model_param[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(model, model_param, time_horizon, algo):\n",
    "    k = len(model_param)\n",
    "    nb_tries = np.zeros(k, int)\n",
    "    cum_rewards = np.zeros(k, float)\n",
    "    action_seq = []\n",
    "    reward_seq = []\n",
    "    for t in range(time_horizon):\n",
    "        a = get_action(algo, cum_rewards, nb_tries)\n",
    "        r = get_reward(a, model, model_param)\n",
    "        nb_tries[a] += 1\n",
    "        cum_rewards[a] += r\n",
    "        action_seq.append(a)\n",
    "        reward_seq.append(r)\n",
    "    return action_seq, reward_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli rewards\n",
    "model = \"bernoulli\"\n",
    "model_param = [0.6, 0.3, 0.1]\n",
    "time_horizon = 10\n",
    "algo = algos[0]\n",
    "\n",
    "action_seq, reward_seq = simulate(model, model_param, time_horizon, algo)\n",
    "print(\"actions = \", action_seq)\n",
    "print(\"rewards = \", [int(r) for r in reward_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal rewards\n",
    "model = \"normal\"\n",
    "model_param = [(2,1), (2.5,1)]\n",
    "algo = algos[0]\n",
    "\n",
    "action_seq, reward_seq = simulate(model, model_param, time_horizon, algo)\n",
    "print(\"actions = \", action_seq)\n",
    "print(\"rewards = \", [np.round(r,1) for r in reward_seq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**To do:**\n",
    "* Write the function `get_metrics` that returns the regret and precision throughout the run of the algorithm.\n",
    "* Observe the behaviour of each algorithm over independent runs, for both models and different instances of the model.\n",
    "* How do you explain that the regret can be negative?\n",
    "\n",
    "**Note:** The `get_best_action` function returns the best action(s) (depending on the model only) and the corresponding expected reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(model, model_param):\n",
    "    if model == \"bernoulli\":\n",
    "        best_reward = np.max(model_param)\n",
    "        best_actions = np.where(model_param == best_reward)[0]\n",
    "    elif model == \"normal\":\n",
    "        rewards = [model_param[a][0] for a in range(len(model_param))]\n",
    "        best_reward = np.max(rewards)\n",
    "        best_actions = np.where(rewards == best_reward)[0]\n",
    "    return best_actions, best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(action_seq, reward_seq, best_actions, best_reward):\n",
    "    regret = np.zeros_like(reward_seq)\n",
    "    precision = np.zeros_like(action_seq)\n",
    "    return regret, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(metrics):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 4))\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Regret')\n",
    "    ax1.plot(range(time_horizon),metrics[0], color = 'b')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_ylim(-0.02,1.02)\n",
    "    ax2.plot(range(time_horizon),metrics[1], color = 'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_horizon = 1000\n",
    "model = \"bernoulli\"\n",
    "model_param = [0.2, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algos[3]\n",
    "results = simulate(model, model_param, time_horizon,  algo)\n",
    "metrics = get_metrics(*results, *get_best_action(model, model_param))\n",
    "show_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##solution de francois \n",
    "def get_metrics(action_seq, reward_seq, best_actions, best_reward):\n",
    "    regret = np.cumsum(best_reward-reward_seq)\n",
    "    precision = np.cumsum(action_seq==best_actions)/np.arange(1, len(action_seq)+1, 1)\n",
    "    return regret, precision\n",
    "\n",
    "def show_metrics(algo_name, metrics, ax1, ax2):\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Regret')\n",
    "    ax1.plot(range(time_horizon),metrics[0], label=algo_name)\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_ylim(-0.02,1.02)\n",
    "    ax2.plot(range(time_horizon),metrics[1], label=algo_name)\n",
    "    \n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20, 7))\n",
    "for algo in algos:\n",
    "    results = simulate(model, model_param, time_horizon,  algo)\n",
    "    metrics = get_metrics(*results, *get_best_action(model, model_param))\n",
    "    show_metrics(algo, metrics, ax1, ax2)\n",
    "plt.legend(bbox_to_anchor=[1.2, 0.5], loc='center', fontsize=13)\n",
    "plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "Finally, we provide some statistics on the performance of each algorithm for different time horizons.\n",
    "\n",
    "**To do:**\n",
    "* Compare the performance of the algorithms.\n",
    "* What algorithm would you recommand for a time horizon $T = 1000$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(nb_samples, time_periods, model, model_param, algo):\n",
    "    time_horizon = max(time_periods)\n",
    "    norm_regret_samples = [[] for t in time_periods]\n",
    "    precision_samples = [[] for t in time_periods]\n",
    "    for s in range(nb_samples):\n",
    "        results = simulate(model, model_param, time_horizon, algo)\n",
    "        regret, precision = get_metrics(*results, *get_best_action(model, model_param))\n",
    "        for i,t in enumerate(time_periods):\n",
    "            norm_regret_samples[i].append(regret[t - 1] / t)\n",
    "            precision_samples[i].append(precision[t - 1])\n",
    "    return norm_regret_samples, precision_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_stats(time_periods, stats):\n",
    "    meanprops = dict(marker='o', markeredgecolor='black', markerfacecolor='r')\n",
    "    medianprops = dict(linestyle='-', linewidth=2.5, color = 'b')\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 4))\n",
    "    ax1.boxplot(stats[0], positions = range(len(time_periods)), showfliers = False, showmeans = True, meanprops = meanprops, medianprops = medianprops)\n",
    "    ax1.axhline(linestyle = '--', color = 'r')\n",
    "    ax1.set_xticklabels(time_periods)\n",
    "    ax1.set_xlabel('Time horizon')\n",
    "    ax1.set_ylabel('Regret per action')\n",
    "    ax2.boxplot(stats[1], positions = range(len(time_periods)), showfliers = False, showmeans = True, meanprops = meanprops, medianprops = medianprops)\n",
    "    ax2.set_ylim(-0.02,1.02)\n",
    "    ax2.axhline(y = 1, linestyle = '--', color = 'r')\n",
    "    ax2.set_xticklabels(time_periods)\n",
    "    ax2.set_xlabel('Time horizon')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_periods = [100,1000,5000]\n",
    "nb_samples = 100\n",
    "model = \"bernoulli\"\n",
    "model_param = [0.1, 0.2]\n",
    "algo = algos[0]\n",
    "stats = get_stats(nb_samples, time_periods, model, model_param, algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean as a red dot, median as a blue line\n",
    "show_stats(time_periods, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
